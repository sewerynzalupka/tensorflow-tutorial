{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Untitled",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "d1DjltuNY5x0",
        "colab_type": "code",
        "outputId": "b5c25a2b-c333-4a5a-8dac-fb3139d34a2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1510
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "work_dir = \"/content/tensorflow-tutorial/\"\n",
        "if os.getcwd() != work_dir:\n",
        "    !git clone https://github.com/sewerynzalupka/tensorflow-tutorial.git\n",
        "os.chdir(work_dir)\n",
        "\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'tensorflow-tutorial'...\n",
            "remote: Enumerating objects: 489, done.\u001b[K\n",
            "remote: Counting objects: 100% (489/489), done.\u001b[K\n",
            "remote: Compressing objects: 100% (261/261), done.\u001b[K\n",
            "remote: Total 489 (delta 233), reused 477 (delta 223), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (489/489), 62.70 MiB | 22.67 MiB/s, done.\n",
            "Resolving deltas: 100% (233/233), done.\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 19)) (1.14.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 20)) (1.1.0)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 21)) (1.0.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 22)) (3.0.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 23)) (4.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 24)) (0.20.3)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 30)) (1.13.1)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter->-r requirements.txt (line 21)) (4.6.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter->-r requirements.txt (line 21)) (7.4.2)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter->-r requirements.txt (line 21)) (4.4.3)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from jupyter->-r requirements.txt (line 21)) (5.4.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter->-r requirements.txt (line 21)) (6.0.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter->-r requirements.txt (line 21)) (5.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 22)) (2.5.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 22)) (1.0.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 22)) (2.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 22)) (0.10.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow->-r requirements.txt (line 23)) (0.46)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 30)) (1.11.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 30)) (0.7.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 30)) (0.7.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 30)) (1.0.7)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 30)) (3.6.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 30)) (0.2.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 30)) (1.1.0)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 30)) (1.13.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 30)) (0.33.1)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 30)) (1.13.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 30)) (1.0.9)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 30)) (1.15.0)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter->-r requirements.txt (line 21)) (5.5.0)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter->-r requirements.txt (line 21)) (4.3.2)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter->-r requirements.txt (line 21)) (5.2.4)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter->-r requirements.txt (line 21)) (4.5.3)\n",
            "Requirement already satisfied: widgetsnbextension~=3.4.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->-r requirements.txt (line 21)) (3.4.2)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->-r requirements.txt (line 21)) (4.4.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->-r requirements.txt (line 21)) (2.1.3)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->-r requirements.txt (line 21)) (4.4.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->-r requirements.txt (line 21)) (0.2.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 21)) (0.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 21)) (2.10)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 21)) (3.1.0)\n",
            "Requirement already satisfied: mistune>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 21)) (0.8.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 21)) (1.4.2)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 21)) (0.5.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->-r requirements.txt (line 21)) (0.4.2)\n",
            "Collecting prompt-toolkit<2.1.0,>=2.0.0 (from jupyter-console->jupyter->-r requirements.txt (line 21))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/a7/9b1dd14ef45345f186ef69d175bdd2491c40ab1dfa4b2b3e4352df719ed7/prompt_toolkit-2.0.9-py3-none-any.whl (337kB)\n",
            "\u001b[K    100% |████████████████████████████████| 337kB 6.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->-r requirements.txt (line 21)) (0.8.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->-r requirements.txt (line 22)) (40.8.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow->-r requirements.txt (line 30)) (2.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow->-r requirements.txt (line 30)) (3.0.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow->-r requirements.txt (line 30)) (0.14.1)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow->-r requirements.txt (line 30)) (2.0.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->-r requirements.txt (line 21)) (4.3.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->-r requirements.txt (line 21)) (0.7.5)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->-r requirements.txt (line 21)) (4.6.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->-r requirements.txt (line 21)) (0.8.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel->jupyter->-r requirements.txt (line 21)) (17.0.0)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter->-r requirements.txt (line 21)) (2.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->nbconvert->jupyter->-r requirements.txt (line 21)) (1.1.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter->-r requirements.txt (line 21)) (0.5.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->jupyter-console->jupyter->-r requirements.txt (line 21)) (0.1.7)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.3.3; sys_platform != \"win32\"->notebook->jupyter->-r requirements.txt (line 21)) (0.6.0)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python3.6/dist-packages (from mock>=2.0.0->tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow->-r requirements.txt (line 30)) (5.1.3)\n",
            "\u001b[31mipython 5.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.4, but you'll have prompt-toolkit 2.0.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: prompt-toolkit\n",
            "  Found existing installation: prompt-toolkit 1.0.15\n",
            "    Uninstalling prompt-toolkit-1.0.15:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.15\n",
            "Successfully installed prompt-toolkit-2.0.9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "prompt_toolkit"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "dPkGC8z8aH9H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ]
    },
    {
      "metadata": {
        "id": "ulDRle68aJ9E",
        "colab_type": "code",
        "outputId": "a337cc47-55ef-47c5-d3b6-31ea5c747fbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import PIL\n",
        "import os\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "# TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Helper libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.13.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iwWladPdazXP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Dataset - SigComp2011"
      ]
    },
    {
      "metadata": {
        "id": "yVWoEapabE_X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Download and extract training and test sets (or use cached files)"
      ]
    },
    {
      "metadata": {
        "id": "qMLxzr6ocMZe",
        "colab_type": "code",
        "outputId": "b2d9acf9-bdff-4923-81d4-78838a6f9962",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "cell_type": "code",
      "source": [
        "import signatures\n",
        "\n",
        "signatures.maybe_download_and_extract()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "- Download progress: 100.0%\n",
            "Download finished. Extracting files.\n",
            "Done.\n",
            "- Download progress: 100.0%\n",
            "Download finished. Extracting files.\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3zQS8vw824_D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3060f9cb-0d4d-4e7e-d21e-4f4658427150"
      },
      "cell_type": "code",
      "source": [
        "!ls data/sigComp2011/trainingSet/OfflineSignatures/Dutch/TrainingSet"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Offline Forgeries'  'Offline Genuine'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lvDPrGA93WO7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This dataset has different directory structure than the Keras API requires, so copy the files into separate directories for the training- and test-sets."
      ]
    },
    {
      "metadata": {
        "id": "bKUsK8Sl3cLT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "c533e9b8-0592-4c1c-b44e-f2fddb1d7893"
      },
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import pickle\n",
        "import cv2\n",
        "\n",
        "def NormalizedImage(img, new_height, new_width):\n",
        "  cImg = 255*np.ones((new_height,new_width,3))\n",
        "  width =  np.size(img,1)\n",
        "  height = np.size(img,0)\n",
        "  left = int((new_width - width)/2.)\n",
        "  top = int(((new_height - height)/2.))\n",
        "  right = int(((width + new_width)/2.))\n",
        "  bottom = int(((height + new_height)/2.))\n",
        "  cImg[top:bottom, left:right,:] = img\n",
        "  return cImg\n",
        "\n",
        "path = os.listdir('data/sigComp2011/trainingSet/OfflineSignatures/Dutch/TrainingSet/Offline Genuine/')\n",
        "path.sort(key=str.lower)\n",
        "\n",
        "print(path[0], path[0][:3])\n",
        "\n",
        "def get_filenames(ext, dir):\n",
        "  filenames = []\n",
        "  for root, dirs, files in os.walk(dir):\n",
        "    for file in files:\n",
        "      if file.lower().endswith(ext.lower()):\n",
        "        filenames.append(file)\n",
        "  return filenames\n",
        "\n",
        "print(get_filenames('.png', 'data/sigComp2011/trainingSet/OfflineSignatures/Dutch/TrainingSet/Offline Genuine/'))\n",
        "\n",
        "import re\n",
        "forgery_filename_pattern = r\"(?P<forger>\\d{4})(?P<signer>\\d{3})_(?P<i>\\d{2})\"\n",
        "genuine_filename_pattern = r\"(?P<signer>\\d{3})_(?P<i>\\d{2})\"\n",
        "\n",
        "signers = set()\n",
        "for filename in get_filenames('.png', 'data/sigComp2011/trainingSet/OfflineSignatures/Dutch/'):\n",
        "#   forgery_match = re.match(forgery_filename_pattern, filename)\n",
        "  genuine_match = re.match(genuine_filename_pattern, filename)\n",
        "#   if forgery_match:\n",
        "#     signers.add(forgery_match.group('signer'))\n",
        "  if genuine_match:\n",
        "    signers.add(genuine_match.group('signer'))\n",
        "    \n",
        "print(sorted(list(signers)))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "001_01.PNG 001\n",
            "['016_12.PNG', '014_17.PNG', '003_04.PNG', '002_20.PNG', '003_10.PNG', '001_13.PNG', '002_12.PNG', '014_16.PNG', '002_16.PNG', '001_05.PNG', '001_20.PNG', '015_05.PNG', '006_14.PNG', '009_11.PNG', '006_08.PNG', '009_05.PNG', '012_12.PNG', '009_22.PNG', '002_21.PNG', '003_18.PNG', '012_05.PNG', '014_04.PNG', '009_24.PNG', '016_16.PNG', '001_15.PNG', '015_08.PNG', '016_02.PNG', '004_22.PNG', '006_01.PNG', '015_11.PNG', '006_11.PNG', '004_21.PNG', '004_13.PNG', '009_20.PNG', '006_23.PNG', '015_07.PNG', '014_02.PNG', '009_15.PNG', '012_20.PNG', '006_21.PNG', '001_14.PNG', '004_19.PNG', '009_04.PNG', '016_05.PNG', '003_08.PNG', '016_07.PNG', '003_20.PNG', '009_21.PNG', '002_18.PNG', '014_07.PNG', '004_08.PNG', '001_08.PNG', '014_18.PNG', '012_11.PNG', '016_06.PNG', '001_04.PNG', '003_17.PNG', '006_17.PNG', '012_13.PNG', '014_21.PNG', '003_13.PNG', '016_01.PNG', '012_09.PNG', '009_14.PNG', '001_17.PNG', '003_16.PNG', '004_23.PNG', '014_11.PNG', '006_20.PNG', '016_13.PNG', '016_15.PNG', '009_08.PNG', '009_02.PNG', '014_06.PNG', '004_05.PNG', '016_18.PNG', '004_04.PNG', '003_22.PNG', '015_24.PNG', '004_12.PNG', '004_18.PNG', '016_08.PNG', '003_06.PNG', '003_12.PNG', '003_14.PNG', '001_11.PNG', '002_13.PNG', '012_01.PNG', '002_02.PNG', '006_15.PNG', '012_14.PNG', '006_13.PNG', '001_06.PNG', '003_19.PNG', '001_07.PNG', '012_10.PNG', '004_11.PNG', '001_02.PNG', '006_04.PNG', '002_08.PNG', '006_09.PNG', '015_17.PNG', '002_22.PNG', '003_23.PNG', '006_10.PNG', '006_03.PNG', '015_02.PNG', '014_13.PNG', '016_09.PNG', '015_22.PNG', '012_02.PNG', '015_16.PNG', '003_02.PNG', '004_02.PNG', '009_23.PNG', '012_03.PNG', '002_17.PNG', '001_24.PNG', '014_08.PNG', '009_07.PNG', '003_15.PNG', '014_22.PNG', '016_17.PNG', '009_16.PNG', '012_19.PNG', '014_24.PNG', '012_15.PNG', '006_22.PNG', '002_23.PNG', '009_01.PNG', '014_09.PNG', '002_14.PNG', '004_03.PNG', '016_22.PNG', '016_14.PNG', '009_03.PNG', '004_07.PNG', '002_19.PNG', '004_06.PNG', '001_10.PNG', '006_18.PNG', '009_19.PNG', '001_23.PNG', '015_04.PNG', '001_01.PNG', '012_04.PNG', '014_12.PNG', '004_14.PNG', '015_21.PNG', '004_20.PNG', '003_05.PNG', '006_12.PNG', '001_18.PNG', '002_06.PNG', '014_05.PNG', '015_15.PNG', '015_09.PNG', '015_01.PNG', '012_06.PNG', '002_10.PNG', '001_03.PNG', '009_06.PNG', '001_12.PNG', '012_16.PNG', '002_01.PNG', '006_24.PNG', '001_19.PNG', '015_19.PNG', '012_22.PNG', '004_24.PNG', '014_10.PNG', '015_10.PNG', '006_19.PNG', '002_15.PNG', '002_03.PNG', '014_14.PNG', '016_04.PNG', '014_19.PNG', '016_21.PNG', '012_18.PNG', '003_01.PNG', '014_23.PNG', '004_16.PNG', '016_03.PNG', '015_23.PNG', '006_16.PNG', '015_06.PNG', '006_02.PNG', '006_05.PNG', '009_17.PNG', '009_10.PNG', '001_21.PNG', '014_03.PNG', '004_01.PNG', '016_20.PNG', '003_09.PNG', '016_23.PNG', '014_15.PNG', '012_08.PNG', '016_24.PNG', '001_22.PNG', '002_11.PNG', '015_14.PNG', '004_15.PNG', '015_03.PNG', '003_11.PNG', '009_09.PNG', '003_07.PNG', '015_12.PNG', '012_23.PNG', '006_07.PNG', '016_11.PNG', '002_09.PNG', '006_06.PNG', '014_01.PNG', '002_05.PNG', '012_07.PNG', '015_13.PNG', '009_18.PNG', '003_21.PNG', '004_10.PNG', '012_24.PNG', '002_04.PNG', '015_20.PNG', '004_17.PNG', '001_16.PNG', '016_10.PNG', '002_24.PNG', '003_24.PNG', '003_03.PNG', '015_18.PNG', '004_09.PNG', '009_13.PNG', '012_21.PNG', '009_12.PNG', '001_09.PNG', '002_07.PNG', '014_20.PNG', '012_17.PNG']\n",
            "['001', '002', '003', '004', '006', '009', '012', '014', '015', '016']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "140j_K1Q-N9b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "dd16d58b-a726-4687-9918-1030a50689bc"
      },
      "cell_type": "code",
      "source": [
        "class SignatureVerificationDataset:\n",
        "    def __init__(self, train_dir, test_dir, exts='.png'):\n",
        "        \"\"\"\n",
        "        Create a data-set consisting of the filenames in the given directory\n",
        "        and sub-dirs that match the given filename-extensions.\n",
        "\n",
        "        :param train_dir:\n",
        "            Root-dir for the files in the data-set.\n",
        "            This would be 'knifey-spoony/' in the example above.\n",
        "            \n",
        "        :param test_dir:\n",
        "            Root-dir for the files in the data-set.\n",
        "            This would be 'knifey-spoony/' in the example above.\n",
        "\n",
        "        :param exts:\n",
        "            String or tuple of strings with valid filename-extensions.\n",
        "            Not case-sensitive.\n",
        "\n",
        "        :return:\n",
        "            Object instance.\n",
        "        \"\"\"\n",
        "\n",
        "        # Extend input directories to the full path.\n",
        "        train_dir = os.path.abspath(train_dir)\n",
        "        test_dir = os.path.abspath(test_dir)\n",
        "\n",
        "        # Input directories.\n",
        "        self.train_dir = train_dir\n",
        "        self.test_dir = test_dir\n",
        "\n",
        "        # Convert all file-extensions to lower-case.\n",
        "        self.exts = tuple(ext.lower() for ext in exts)\n",
        "\n",
        "        # Names for the classes.\n",
        "        self.class_names = []\n",
        "\n",
        "        # Filenames for all the files in the training-set.\n",
        "        self.filenames = []\n",
        "\n",
        "        # Filenames for all the files in the test-set.\n",
        "        self.filenames_test = []\n",
        "\n",
        "        # Class-number for each file in the training-set.\n",
        "        self.class_numbers = []\n",
        "\n",
        "        # Class-number for each file in the test-set.\n",
        "        self.class_numbers_test = []\n",
        "\n",
        "        # Total number of classes in the data-set.\n",
        "        self.num_classes = 0\n",
        "\n",
        "        # For all files/dirs in the input directory.\n",
        "        for name in os.listdir(in_dir):\n",
        "            # Full path for the file / dir.\n",
        "            current_dir = os.path.join(in_dir, name)\n",
        "\n",
        "            # If it is a directory.\n",
        "            if os.path.isdir(current_dir):\n",
        "                # Add the dir-name to the list of class-names.\n",
        "                self.class_names.append(name)\n",
        "\n",
        "                # Training-set.\n",
        "\n",
        "                # Get all the valid filenames in the dir (not sub-dirs).\n",
        "                filenames = self._get_filenames(current_dir)\n",
        "\n",
        "                # Append them to the list of all filenames for the training-set.\n",
        "                self.filenames.extend(filenames)\n",
        "\n",
        "                # The class-number for this class.\n",
        "                class_number = self.num_classes\n",
        "\n",
        "                # Create an array of class-numbers.\n",
        "                class_numbers = [class_number] * len(filenames)\n",
        "\n",
        "                # Append them to the list of all class-numbers for the training-set.\n",
        "                self.class_numbers.extend(class_numbers)\n",
        "\n",
        "                # Test-set.\n",
        "\n",
        "                # Get all the valid filenames in the sub-dir named 'test'.\n",
        "                filenames_test = self._get_filenames(os.path.join(current_dir, 'test'))\n",
        "\n",
        "                # Append them to the list of all filenames for the test-set.\n",
        "                self.filenames_test.extend(filenames_test)\n",
        "\n",
        "                # Create an array of class-numbers.\n",
        "                class_numbers = [class_number] * len(filenames_test)\n",
        "\n",
        "                # Append them to the list of all class-numbers for the test-set.\n",
        "                self.class_numbers_test.extend(class_numbers)\n",
        "\n",
        "                # Increase the total number of classes in the data-set.\n",
        "                self.num_classes += 1\n",
        "                \n",
        "    def _get_filenames(self, dir):\n",
        "        \"\"\"\n",
        "        Create and return a list of filenames with matching extensions in the given directory.\n",
        "\n",
        "        :param dir:\n",
        "            Directory to scan for files. Sub-dirs are not scanned.\n",
        "\n",
        "        :return:\n",
        "            List of filenames. Only filenames. Does not include the directory.\n",
        "        \"\"\"\n",
        "        filenames = []\n",
        "        for root, dirs, files in os.walk(dir):\n",
        "          for file in files:\n",
        "            if file.lower().endswith(self.exts):\n",
        "              filenames.append(os.path.join(root, file))\n",
        "        return filenames\n",
        "\n",
        "\n",
        "ds = SignatureVerificationDataset(test_dir='data/sigComp2011/Testdata_SigComp2011/SigComp11-Offlinetestset/Dutch/', train_dir='data/sigComp2011/trainingSet/OfflineSignatures/Dutch/TrainingSet/')\n",
        "print(ds.class_names, ds.filenames, ds.num_classes)\n",
        "# base_model = InceptionV3(include_top = False, weights='imagenet', input_shape= (551,1117,3))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Offline Genuine', 'Offline Forgeries'] ['016_12.PNG', '014_17.PNG', '003_04.PNG', '002_20.PNG', '003_10.PNG', '001_13.PNG', '002_12.PNG', '014_16.PNG', '002_16.PNG', '001_05.PNG', '001_20.PNG', '015_05.PNG', '006_14.PNG', '009_11.PNG', '006_08.PNG', '009_05.PNG', '012_12.PNG', '009_22.PNG', '002_21.PNG', '003_18.PNG', '012_05.PNG', '014_04.PNG', '009_24.PNG', '016_16.PNG', '001_15.PNG', '015_08.PNG', '016_02.PNG', '004_22.PNG', '006_01.PNG', '015_11.PNG', '006_11.PNG', '004_21.PNG', '004_13.PNG', '009_20.PNG', '006_23.PNG', '015_07.PNG', '014_02.PNG', '009_15.PNG', '012_20.PNG', '006_21.PNG', '001_14.PNG', '004_19.PNG', '009_04.PNG', '016_05.PNG', '003_08.PNG', '016_07.PNG', '003_20.PNG', '009_21.PNG', '002_18.PNG', '014_07.PNG', '004_08.PNG', '001_08.PNG', '014_18.PNG', '012_11.PNG', '016_06.PNG', '001_04.PNG', '003_17.PNG', '006_17.PNG', '012_13.PNG', '014_21.PNG', '003_13.PNG', '016_01.PNG', '012_09.PNG', '009_14.PNG', '001_17.PNG', '003_16.PNG', '004_23.PNG', '014_11.PNG', '006_20.PNG', '016_13.PNG', '016_15.PNG', '009_08.PNG', '009_02.PNG', '014_06.PNG', '004_05.PNG', '016_18.PNG', '004_04.PNG', '003_22.PNG', '015_24.PNG', '004_12.PNG', '004_18.PNG', '016_08.PNG', '003_06.PNG', '003_12.PNG', '003_14.PNG', '001_11.PNG', '002_13.PNG', '012_01.PNG', '002_02.PNG', '006_15.PNG', '012_14.PNG', '006_13.PNG', '001_06.PNG', '003_19.PNG', '001_07.PNG', '012_10.PNG', '004_11.PNG', '001_02.PNG', '006_04.PNG', '002_08.PNG', '006_09.PNG', '015_17.PNG', '002_22.PNG', '003_23.PNG', '006_10.PNG', '006_03.PNG', '015_02.PNG', '014_13.PNG', '016_09.PNG', '015_22.PNG', '012_02.PNG', '015_16.PNG', '003_02.PNG', '004_02.PNG', '009_23.PNG', '012_03.PNG', '002_17.PNG', '001_24.PNG', '014_08.PNG', '009_07.PNG', '003_15.PNG', '014_22.PNG', '016_17.PNG', '009_16.PNG', '012_19.PNG', '014_24.PNG', '012_15.PNG', '006_22.PNG', '002_23.PNG', '009_01.PNG', '014_09.PNG', '002_14.PNG', '004_03.PNG', '016_22.PNG', '016_14.PNG', '009_03.PNG', '004_07.PNG', '002_19.PNG', '004_06.PNG', '001_10.PNG', '006_18.PNG', '009_19.PNG', '001_23.PNG', '015_04.PNG', '001_01.PNG', '012_04.PNG', '014_12.PNG', '004_14.PNG', '015_21.PNG', '004_20.PNG', '003_05.PNG', '006_12.PNG', '001_18.PNG', '002_06.PNG', '014_05.PNG', '015_15.PNG', '015_09.PNG', '015_01.PNG', '012_06.PNG', '002_10.PNG', '001_03.PNG', '009_06.PNG', '001_12.PNG', '012_16.PNG', '002_01.PNG', '006_24.PNG', '001_19.PNG', '015_19.PNG', '012_22.PNG', '004_24.PNG', '014_10.PNG', '015_10.PNG', '006_19.PNG', '002_15.PNG', '002_03.PNG', '014_14.PNG', '016_04.PNG', '014_19.PNG', '016_21.PNG', '012_18.PNG', '003_01.PNG', '014_23.PNG', '004_16.PNG', '016_03.PNG', '015_23.PNG', '006_16.PNG', '015_06.PNG', '006_02.PNG', '006_05.PNG', '009_17.PNG', '009_10.PNG', '001_21.PNG', '014_03.PNG', '004_01.PNG', '016_20.PNG', '003_09.PNG', '016_23.PNG', '014_15.PNG', '012_08.PNG', '016_24.PNG', '001_22.PNG', '002_11.PNG', '015_14.PNG', '004_15.PNG', '015_03.PNG', '003_11.PNG', '009_09.PNG', '003_07.PNG', '015_12.PNG', '012_23.PNG', '006_07.PNG', '016_11.PNG', '002_09.PNG', '006_06.PNG', '014_01.PNG', '002_05.PNG', '012_07.PNG', '015_13.PNG', '009_18.PNG', '003_21.PNG', '004_10.PNG', '012_24.PNG', '002_04.PNG', '015_20.PNG', '004_17.PNG', '001_16.PNG', '016_10.PNG', '002_24.PNG', '003_24.PNG', '003_03.PNG', '015_18.PNG', '004_09.PNG', '009_13.PNG', '012_21.PNG', '009_12.PNG', '001_09.PNG', '002_07.PNG', '014_20.PNG', '012_17.PNG', '0110002_03.png', '0117009_03.png', '0214014_03.png', '0205006_02.png', '0202016_03.png', '0108002_02.png', '0127016_02.png', '0110016_01.png', '0210015_01.png', '0103004_04.png', '0117009_04.png', '0214014_02.png', '0102014_01.png', '0210015_04.png', '0118002_01.png', '0110002_02.png', '0201001_02.png', '0210012_02.png', '0118002_03.png', '0113012_01.png', '0202006_01.png', '0111006_01.png', '0126003_01.png', '0124004_03.png', '0123009_01.png', '0106015_01.png', '0102014_03.png', '0106015_04.png', '0124004_01.png', '0104014_03.png', '0213015_02.png', '0206003_01.png', '0102014_02.png', '0105004_01.png', '0127016_04.png', '0107016_03.png', '0206012_04.png', '0123009_02.png', '0214014_01.png', '0117009_02.png', '0201009_04.png', '0111006_03.png', '0121003_02.png', '0201001_01.png', '0106015_03.png', '0108002_01.png', '0210012_04.png', '0201001_03.png', '0123009_04.png', '0213015_03.png', '0121003_03.png', '0213015_04.png', '0104014_01.png', '0202016_01.png', '0103004_03.png', '0126003_03.png', '0121003_01.png', '0110016_03.png', '0208014_01.png', '0111006_02.png', '0127016_01.png', '0119001_01.png', '0127016_03.png', '0202016_02.png', '0123009_03.png', '0113012_03.png', '0202006_02.png', '0201001_04.png', '0205006_01.png', '0126003_04.png', '0124004_02.png', '0107016_04.png', '0118002_04.png', '0119001_03.png', '0210012_03.png', '0202006_03.png', '0210015_03.png', '0108002_03.png', '0110002_04.png', '0201009_03.png', '0105004_03.png', '0105004_04.png', '0126003_02.png', '0208014_02.png', '0104014_02.png', '0110016_04.png', '0124004_04.png', '0210012_01.png', '0119001_02.png', '0202016_04.png', '0113012_02.png', '0113012_04.png', '0202006_04.png', '0206003_02.png', '0205006_03.png', '0206003_04.png', '0103004_02.png', '0208014_03.png', '0214014_04.png', '0201009_01.png', '0210015_02.png', '0106015_02.png', '0206012_01.png', '0206012_03.png', '0110016_02.png', '0213015_01.png', '0117009_01.png', '0108002_04.png', '0206003_03.png', '0105004_02.png', '0206012_02.png', '0111006_04.png', '0110002_01.png', '0119001_04.png', '0208014_04.png', '0121003_04.png', '0104014_04.png', '0201009_02.png', '0205006_04.png', '0102014_04.png', '0107016_01.png', '0107016_02.png', '0118002_02.png'] 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "p3rEolQuE4GD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}